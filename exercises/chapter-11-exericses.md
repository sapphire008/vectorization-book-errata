1. What are the differences between encoder and decoder architecture in the transformer model?
2. What is Cloze task? How is it being used to pre-train the language model?
3. What are the differences between Pre-LN and Post-LN architecture?
4. Compare and contrast batch normalization and layer normalization.
5. How does RMSNorm differ from LayerNorm?
6. What are the two formulations of the sinusoidal positional encoding?
7. How does the idea of sinusoidal positional encoding differ from that of rotary positional encoding?
8. Name several activation functions used in the feedforward layer of the transformer model.
9. What are topics of discussion with respect to AI safety and alignment?
10. Can you identify some inefficiencies in the tiny LLaMA implementation?
